<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>Notes | Saeran Vasanthakumar</title>
    <link rel="stylesheet" type="text/css" href="/static/screen.css" media="screen" />
    <meta name="author" content="Saeran Vasanthakumar">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ['\(','\)'] ],
                displayMath: [ ['$$','$$'], ['\[','\]']],
                skipTags: ["script","noscript","style","textarea", "code", "img src"],
                processClass: 'content',
                processEscapes: true
            },
            "HTML-CSS": { availableFonts: ["TeX"] }
        });
    </script>
</head>
<body>
<span class="content">
    <span class="nbk_title"><a href="/">Notes</a></span>
    <span class="nbk_comment"></span>
    <span class="nbk_comment">by Saeran Vasanthakumar</span>
    <span class="_doc_sep">---</span>
    
    <span class="nbk_directive"><i>20/09/22 - Linear Regression as Geometric Projection [Draft]</i></span>
    <!-- autoescape off to allow images. Debug why safe filter isn't working -->
    
    Warning: This is a draft.
    
    This note illustrates a geometric interpretation of multivariate linear regression,
    which I have found to be the most intuitive way to think of regression problems.
    
    Given a feature matrix X, and output vector y, the objective of linear regression is
    to calculate a set of coefficients B that when linearly combined with X produces a
    new vector \(\hat{y}\).
    
    \( \hat{y} = X B \)
    
    Think of X as a m x n matrix, and y as a n-dimensional column vector.
    
    \( \hat{y} = X B =
    \begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    a_{m,1} & a_{m,2} & \cdots & a_{m,n}
    \end{bmatrix}\\
    \cdot
    \begin{bmatrix}
    a_{1,1} \\
    a_{2,1} \\
    ..  \\
    a_{m,1} \end{bmatrix}\)
    
    Geometrically, one can think of the output vector y as belonging to a m-dimensional
    ambient space. Since m is greater then the rank of X, the feature matrix X contains n
    column vectors belonging to some hyperplane in m-dimensional space. The picture below
    illustrates a y vector in 3-dimensional (m-dimensional) ambient space, and 2 (n)
    3-dimensional column vectors that do not span the 3 dimensional space, since they
    contain a rank of 2.
    
    <img src="/static/images/geometry_regression_2.gif" alt="" width="300">
    
    Therefore, yhat usually does not equal y, because feature matrix X does not contain
    enough linearly independant vectors to solve the X B system. \(\hat{y}\) therefore
    only represents the best prediction of y. It can be thought of as a noisy
    representation of \(y\) that can be summed with an error term to equal y.
    
    \( y = \hat{y} + e \)
    \( y = X B + e \)
    
    In the context of regression, the normal equation \(B = (X^T X)^{-1} X^T y\) is used
    to calculate a matrix \(B\) of coefficients that minimize the sum of square
    differences between the feature matrix \(X\) and predictor vector \(y\). The linear
    combination of \(B\) and (\X\) thus produces the predictor vector \(\hat{y}\).
    
    The geometric perspective thus shows that the residual \(e = y - XB\) is
    perpendicular to \(\hat{y}\). Thus:
    
    \( X.T (y - X B) = 0 \)
    \( X.T (y - \hat{y}) = 0 \)
    
    Solve for B.
    
    \( X.T y - X.T X B = 0 \)
    \(X.T y = X.T X B \)
    \( (X.T X)-1 X.T y = (X.T X)-1 (X.T X) B \)
    \( (X.T X)-1 X.T y = B \)
    \( B = (X.T X)-1 X.T y \)
    
    Multiply \(X\) by \(B\) to get \(\hat{y}\).
    
    Since \(\hat{y} = X B$\)
    \( \hat{y} = X (X.T X)-1 X.T y \)
    \( \hat{y} = P y \)
    
    P is a matrix that projects n-dimensional \(y\) onto n-1 dimensional \(X\)
    hyperplane.
    
    \( P = X (X.T X)-1 X.T \)
    \(    = X B \)
    
    In the geometric context the normal equation is a component of the projection of
    predictor \(y\) onto feature subspace \(X\) to get \(\hat{y}\).
    
<body>
    <span class="_doc_sep">---</span>
    <span class="nbk_grey">email: saeranv @ gmail dot com</span>
    <span class="nbk_grey">git: <a href="https://github.com/saeranv/" style="color:grey;">github.com/saeranv</a></span>
</body>

</span>
</body>
</html>